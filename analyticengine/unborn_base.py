from collections import defaultdict, namedtuple
from copy import deepcopy
import logging
import math
from functools import reduce
import re

from aviso.utils import mathUtils
from aviso.utils.dateUtils import datetime2xl, current_period
# from analyticengine.forecast_base import ForecastBaseModel
import numpy as np
import pandas as pd

from django.utils.functional import cached_property

STATUTE_OF_LIMITATIONS = None

logger = logging.getLogger('gnana.%s' % __name__)


def merge_dict_of_dicts(res1, res2):
    res = {}
    all_dd = list(set(list(res1.keys()) + list(res2.keys())))
    for dd in all_dd:
        res[dd] = {}
        res[dd].update(res1.get(dd, {}))
        res[dd].update(res2.get(dd, {}))
    return res

class TSAggregator(object):

    '''
        Encapsulate the aggregation of multiple time series in this class.
        data will be kept in a dict so it can be constructed efficiently
        but once no more data is expected, it will be converted into a
        more efficient structure showing cumulative sum
    '''

    def __init__(self):
        self._sealed = False
        self.data = {}

    def add(self, *args):
        if len(args) == 1:
            if not isinstance(args[0], TSAggregator):
                data = args[0].data
            elif isinstance(args[0], dict):
                data = args[0]
            else:
                raise Exception("Single argument to TSAggregator.add must be TSAggregator or dict")
            for fld, ts_map in data.items():
                if fld not in self.data:
                    self.data[fld] = deepcopy(ts_map)
                else:
                    for ts, val in ts_map.items():
                        self.data[fld][ts] = self.data[fld].get(ts, 0) + val
        elif len(args) == 2:
            fld, tv = args
            ts_map = self.data.get(fld, None)
            if ts_map is None:
                ts_map = {}
                self.data[fld] = ts_map
            t, v = tv
            ts_map[t] = ts_map.get(t, 0) + v
        else:
            raise Exception("TSAggregator.add accepts maximum of 2 args, you provided %s" % (args))

    def _raise_seal_exception(self):
        raise Exception("Cannot add to sealed TSAggregator")

    def seal_for_query(self):
        if self._sealed:
            return
        for fld in list(self.data.keys()):
            series = np.array(sorted(self.data[fld].items()))
            # do a reverse cumsum on the value
            series[:, 1] = np.cumsum(series[:, 1][::-1])[::-1]
            self.data[fld] = series
        self.add = self._raise_seal_exception
        self._sealed = True

    def get_as_of(self, fld, as_of):
        '''
        get correct as_of value for a cumsum. Note that we have to get next valid value
        not previous valid value as we usually do, but it is a bit tricky since searchsorted's
        options always count the perfect match along with perfect match plus epsilon, but we need
        to distinguish between these cases since we are doing next valid value.
        if side == 'left', then perfect match will give index of perfect match and will not change
            until next largest entry
        if side == 'right' then perfect match will give index of perfect match plus one and
            will not change until next largest entry
        '''
        series = self.data[fld]
        # WARNING: np.searchsorted has a bug when elements are integer, it could give as index
        # which is larger than the length (i.e. implying two elements out)... avoid corner cases
        if as_of <= series[0, 0]:
            return series[0, 1]
        if as_of > series[-1, 0]:
            return 0
        # not index is greater than zero and at most
        idx = np.searchsorted(series[:, 0], as_of, side='right')
        # handle perfect match case
        if series[idx - 1, 0] == as_of:
            return series[idx - 1, 1]
        return series[idx, 1]

    def __str__(self):
        return '\n<TSAggregator>: ' + '\n'.join(["'%s' : %s " % (fld, tv_map)
                                                 for fld, tv_map in self.data.items()]) + '\n'

    __repr__ = __str__


class UnbornBaseModel:

    '''
    Provide an estimate of how much revenue can be generated by the
    opportunities created in the remaining period

    Configuration

        Requires the similar configuration as the forecast for win and loss stages,
        and uses similar time line parameters.

        Algorithm Parameters

            num_cycles: how many cycles to look at in the history

            model_scheme: is a map with following possible keys:
                name: [type=basestring, default=growth_extrap]
                    mean: take average of past num_cyles periods
                    cust_mults: will multiply each period's hist_as_of value by a multiplier and
                        divide the sum by num_cycles. (will refer to cust_mults)
                    growth_extrap: will calculate a top level growth and apply and
                        apply it to each sub-level based on values from previous period
                    seasonal_growth: will estimate the cycle-over-cycle growth rate, then multiply
                        on-season actuals by the estimated growth rate
                cust_mults: [type=dict, required when name==cust_mults]
                    a dictionary mapping relative historic period to a float which will be multiplied
                    by its win amount before average is taken
                growth_model: [type=basestring, default='linear' when model_scheme not provided]
                    geo_mean: using each historic period total from beginning of the period
                        take the geometric mean of period over period growth
                    linear: using each historic period total from beginning of
                        the period, fit a linear model and calculate expected top level value
                        for current period, and use the growth rate implied from it
                growth_anchor_time: [type=basestring, default=bop when model_scheme not provided]
                    bop: use begin of period as the point to be used for calculating growth
                    as_of: use as_of as period the point to be usef for calculating growth

            redist_scheme: is a map with following possible keys:
                name: [type=basestring, default=none]
                    none: do not do any redistribution
                    drop: drop them without redistribution
                    proportional: redistribute proportionally: requires additional parameters
                prop_fld:[type=basestring, default=amount if redist_scheme not provided]
                    Determines whether amount or count should be used from the active opps
                    for calculating what proportion of the old drilldowns they should get.
                redist_hierarchy: [type=basestring]
                    Is tilde-separated filed names which must be a subset of all the groupby fields.
                    If evaluates to False, then, redistribution will happen proportionally among
                    all opps. If a hierarchy is provided, will make best effort to only distribute among
                    buckets which best match the old bucket. (e.g. If some guy leaves his team,
                    we will try to distribute all this forecast between his team mates. If his
                    team is also gone, we will try to distribute among his old colleagues, and if the
                    his entire team is also gone, we will go one level up from there and so on).
                no_new_drilldown: [type=boolean, default=False]
                    if True, will restrict drilldowns to only those available in the historic set

            bands_scheme: is a map with the following keys:
                name: [type=basestring, default=std_mults]
                    std_mults: a multiple of the standard deviation of the win amounts for that
                        group over the past num_cycle periods. (will refer to std_mults)
                    fixed_props: upper and lower bands are defined as a fixed proportion
                        of the value for that drilldown (will refer to fixed_props)
                std_mults: [type=list, default=[1.2815515655446004, 1.2815515655446004] when bands_scheme not provided]
                    is a list of two floats, first for lower band, second for upper band. The
                    multipliers are multiplied by the std of the historic amounts before being
                    subtracted and added respectively from the mean
                fixed_props: [type=list]
                    is a list of two floats, first for lower and second for upper. Each is multiplied
                    directly with the mean to get lower and upper bands.

            track_wins_for_hierarchy: [type=boolean]
                flag stating whether won deals should be tracked for the hierarchy

            track_losses_for_hierarchy: [type=boolean]
                boolean flag stating whether lost deals should be tracked for the hierarchy

           debug_dims: [type=list of lists]
               list of lists where each inner list is made of two entries: the first entry is the tild_separated
               dimension field names and the second entry is the regex to match the dimension values against.
               This will cause DEBUG_INFO to appear as a key under the top-level result.
        config:
            master_buffer_size
            statute_of_limitations : (Optional) restrict the unborn by created date in hist_horz + statute_of_limitations
                (doesn't bound it with horizon if statute_of_limitations is None)
            uip_projections: [type=boolean, default=False]
            flag stating whether unborn model query only rqeruired fields from db
        Field Map:
            terminal_fate
            acv
            created_date (optional, will fall back to record.created_date if missing)
            ts_fld_for_record_created_date :(Default : u'Stage') to get the real_created_date of record, when the record is actually been
                created in CRM
    '''
    allow_aggregation = True
    predicts = True
    compares_with_history = False
    produce_individual_result = False
    combine_prefix = 'ubf'

    def __init__(self, field_map, config, time_horizon, dimensions):
        super(UnbornBaseModel, self).__init__(field_map, config, time_horizon, dimensions)
        self.check_sanity = self.forecast_params.get('check_sanity', True)
        self.th = time_horizon
        self.produce_existing_results = self.th.as_ofF < self.th.beginsF
        as_of_period = current_period(self.th.as_of).mnemonic
        try:
            self.model_scheme = self.forecast_params[as_of_period]['model_scheme']
        except KeyError:
            self.model_scheme = self.forecast_params.get(
                'model_scheme', {
                    'name': 'growth_extrap',
                    'growth_model': 'linear',
                    'growth_anchor_time': 'bop',
                }
            )
        self.redist_scheme = self.forecast_params.get('redist_scheme', {'name': 'none'})
        self.bands_scheme = self.forecast_params.get(
            'bands_scheme', {
                'name': 'std_mults',
                'std_mults': [1.2815515655446004, 1.2815515655446004]
            }
        )

        self.runtime_params = {}
        self.track_wins_for_hierarchy = self.forecast_params.get('track_wins_for_hierarchy', False)
        self.track_losses_for_hierarchy = self.forecast_params.get('track_losses_for_hierarchy', False)
        self.num_cycles = self.forecast_params.get('num_cycles', 4)
        if 'terminal_fate' not in field_map:
            logger.warning("WARNING: UnbornBase model will default to 'terminal_fate' for terminal_fate_fld")
        self.terminal_fate_fld = field_map.get('terminal_fate', 'terminal_fate')
        self.created_date_fld = field_map.get('created_date', None)
        self.first_ts_as_created_date_fld = field_map.get('first_ts_as_created_date', None)
        self.ts_for_record_created_date_fld = field_map.get('ts_fld_for_record_created_date', 'StageTrans')
        self.statute_of_limitations = self.config.get('statute_of_limitations', STATUTE_OF_LIMITATIONS)
        self.uip_projections = self.config.get('uip_projections', False)
        if self.first_ts_as_created_date_fld and self.created_date_fld:
            logger.warning("WILL USE FIRST TIMESTAMP OF '%s' AS THE CREATED DATE OF RECORD",
                           self.first_ts_as_created_date_fld)
        self.amt_fld = field_map['acv']
        self.model_dim_flds = ('RelPrd', 'Existing')
        self.model_dim_types = {'RelPrd': int, 'Existing': bool}
        self.all_dim_flds = self.drilldown_flds + self.model_dim_flds
        self.debug_dims = []
        self.verbose_mode = self.forecast_params['verbose_mode']
        for dim_flds_str, dim_val_match_expr in self.forecast_params.get('debug_dims', []):
            dim_flds = dim_flds_str.split('~')
            dim_idxs = [self.all_dim_flds.index(x) for x in dim_flds]
            self.debug_dims.append((dim_flds_str, dim_val_match_expr, dim_idxs))
        if self.debug_dims:
            self.debug_map = defaultdict(lambda: defaultdict(list))

        self.hist_aggs = defaultdict(TSAggregator)
        self.actv_aggs = defaultdict(TSAggregator)
        self.setup_period_infos()
        self.ObsTuple = namedtuple('ObsTuple', ['period_idx', 'category', 'amount'])

    def determine_deps(self):
        if self.model_scheme['name'] in ['precomputed_mults', 'precomp_with_regional']:
            # TODO: Make this a thing.
            return {'epf': None}
        else:
            return {}

    def incorporate_deps(self, dep_results):
        if self.model_scheme['name'] in ['precomputed_mults', 'precomp_with_regional']:
            epf_res = dep_results['epf']['unborn_stuff']
            self.runtime_params['raw_ubf'] = epf_res['raw']
            self.runtime_params['scaled_ubf'] = epf_res['scaled']

    def field_requested(self):
        """
        Make a list of reqired fields.
        """
        if not self.uip_projections:
            return None
        drilldown_fld_cfg = self.drilldown_config.get('fld_cfg', {})
        drilldown_src_fld = [fld for fld, source in drilldown_fld_cfg.items() if source.get('src', 'S') =='S']
        required_fld = set(drilldown_src_fld + list(self.field_map.values()))
        return list(required_fld)

    def get_batch_task_criteria(self):
        """
        Setup record filter for on MongoDB query.

        For Unborn, we only examine records which were not closed before the minimum date.
        (note deals that have not terminated will have the maximally large terminal date)

        """
        times_of_interest = [t for triplet in self.period_infos for t in triplet]

        lower_bound = min(times_of_interest)

        if self.created_date_fld == 'UIPCreatedDate' and not self.first_ts_as_created_date_fld:
            end_last_hist = self.period_infos[1][2]
            min_curr_period = min(self.period_infos[0][0], self.period_infos[0][1])
            return {'$or': [{'$and': [{'object.created_date': {'$gte': lower_bound}},
                                      {'object.terminal': {'$lte': end_last_hist}}]},
                            {'object.terminal_date': {'$gte': min_curr_period}}]}
        else:
            return {'object.terminal_date': {'$gte': lower_bound}}

    def setup_period_infos(self):
        '''
        keep a clean map of the current and historic as_ofs and horizons
        relies on the timeline setup from the forecast_base class
        '''
        if not self.history_timeline_details:
            self.setuptimeline()
        # the first period will be kept from begin to the as_of since we want
        # to build the existing pipe hierarchy from it as a guide

        self.period_infos = [[self.th.beginsF, self.th.as_ofF, self.th.horizonF]]
        # all historic period will be from hist_as_of to hist_horizon
        for time_point in self.history_timeline_details:
            # the first 3 numbers are his_bigin, hist_as_of and hist_horizon
            self.period_infos.append(([x['xl_date'] for x in time_point]))

    def prepare(self, record):
        '''
        Put a new field on the record called 'observations' which is a list
        filled with named tuples of the form,

            (period_idx, category, amount, count).

        period_idx :: Int
            Integer that corresponds to the periods found in self.period_infos.
        category :: String
            'HE' ::  Historic Existing. These are opportunities that were created after
                    the relative as_of but before the relative begin and won before the relative horizon in
                    some historic period.
            'HN' ::  Historic New. These are opportunities that were created after
                    the relative as_of and the relative begin and won before the relative horizon in
                    some historic period. Most runs will only have H and no HE, since begin<=as_of
            'C' ::  Current. These are opportunities that are born after the
                    current begin. These opportunities are used to detect the
                    current quarter's hierarchy so that we can properly
                    redistribute amounts.
            'I' ::  Ignored. They don't fall in the above groups.
        amount :: Float
            The amount the opportunitiy won for.
        count :: Int
            Is 1 or 0.

        At the end, note that the record itself will also get the category field
        added onto it as well. The rules for this is slightly different than for
        how it is done for observation.
        '''
        self._set_created_date(record)
        terminal_fate, terminal_date = self._get_terminal_fate_and_date(record)

        observations = []

        for i, period in enumerate(self.period_infos):
            hist_begin, hist_as_of, hist_horizon = period[0], period[1], period[2]

            obs_category = 'I'
            if i == 0:
                if self._is_current(record.created_date, terminal_date, terminal_fate):
                    obs_category = 'C'
            elif self._is_historic_existing_unborn(record.created_date, record.real_created_date, terminal_date,
                                                   terminal_fate, hist_begin, hist_as_of, hist_horizon,
                                                   self.statute_of_limitations):
                obs_category = 'HE'
            elif self._is_historic_new_unborn(record.created_date, record.real_created_date, terminal_date,
                                              terminal_fate, hist_begin, hist_as_of, hist_horizon,
                                              self.statute_of_limitations):
                obs_category = 'HN'
            elif self._is_historic_period_new_unborn(record.created_date, record.real_created_date, terminal_date,
                                                     terminal_fate, hist_begin, hist_as_of, hist_horizon,
                                                     self.statute_of_limitations):
                obs_category = 'HPN'

            if obs_category == 'I':
                continue

            amount = 0.0
            if obs_category in ['HN', 'HE', 'HPN']:
                try:
                    amount = mathUtils.excelToFloat(record.getLatest(self.amt_fld))
                except:
                    amount = 0.0
            elif obs_category == "C":
                if terminal_fate == 'W':
                    try:
                        amount = mathUtils.excelToFloat(record.getLatest(self.amt_fld))
                    except:
                        amount = 0.0
                else:
                    try:
                        amount = mathUtils.excelToFloat(record.getAsOfDateF(self.amt_fld, self.time_horizon.as_ofF))
                    except:
                        amount = 0.0

            observations.append(self.ObsTuple(i, obs_category, amount))
        record.observations = observations
        record.category = 'I'
        if record.observations != []:
            record.category = 'H'
            for obs in record.observations:
                if obs.category == 'C':
                    record.category = 'C'

    def _set_created_date(self, record):
        if self.first_ts_as_created_date_fld:
            try:
                record.created_date = float(record.featMap[self.first_ts_as_created_date_fld][0][0])
            except:
                logger.warning("unable to set created_date from first timestamp of %s for %s," +
                               " falling back to built-in record.created_date", self.created_date_fld, record.ID)
        elif self.created_date_fld:
            try:
                record.created_date = float(record.getLatest(self.created_date_fld))
            except:
                logger.warning("unable to set created_date from %s for %s," +
                               " falling back to built-in record.created_date", self.created_date_fld, record.ID)
        try:
            record.real_created_date = float(record.featMap[self.ts_for_record_created_date_fld][0][0])
        except:
            record.real_created_date = record.created_date
            logger.warning("unable to set real_created_date from first timestamp of %s for %s," +
                           " falling back to built-in record.created_date", self.ts_for_record_created_date_fld, record.ID)

    def _get_terminal_fate_and_date(self, record):
        terminal_date = record.featMap.get(self.terminal_fate_fld, None)
        tf_slice = record.getHistorySlice(
            feat=self.terminal_fate_fld,
            beginF=None,
            endF=self.time_horizon.as_ofF,
            honor_created_date=False,  # make sure adjustments are not wiped because of created_date
        )

        if tf_slice != []:
            terminal_fate = tf_slice[-1][1]
            terminal_date = tf_slice[-1][0]
        else:
            terminal_fate = None
            terminal_date = None

        return terminal_fate, terminal_date

    def _is_historic_existing_unborn(self, created_date, real_created_date, terminal_date, terminal_fate, hist_begin,
                                     hist_as_of, hist_horizon, statute_of_limitations):
        if terminal_fate == 'W':
            if statute_of_limitations is None:
                if hist_as_of < created_date <= hist_begin:
                    if hist_begin <= terminal_date <= hist_horizon:
                        return True
            else:
                if hist_as_of < created_date <= hist_begin and real_created_date < hist_begin + statute_of_limitations:
                    if hist_begin <= terminal_date <= hist_horizon:
                        return True

        return False

    def _is_historic_new_unborn(self, created_date, real_created_date, terminal_date, terminal_fate, hist_begin,
                                hist_as_of, hist_horizon, statute_of_limitations):
        if terminal_fate == 'W':
            if statute_of_limitations is None:
                if hist_as_of < created_date and hist_begin < created_date:
                    if hist_begin <= terminal_date <= hist_horizon:
                        return True
            else:
                if hist_as_of < created_date and hist_begin < created_date and \
                        real_created_date < hist_horizon + statute_of_limitations:
                    if hist_begin <= terminal_date <= hist_horizon:
                        return True

        return False

    def _is_historic_period_new_unborn(self, created_date, real_created_date, terminal_date, terminal_fate, hist_begin,
                                       hist_as_of, hist_horizon, statute_of_limitations):
        if terminal_fate == 'W':
            if statute_of_limitations is None:
                if hist_as_of >= created_date and hist_begin < created_date:
                    if hist_begin <= terminal_date <= hist_horizon:
                        return True
            else:
                if hist_as_of >= created_date and hist_begin < created_date and \
                        real_created_date < hist_horizon + statute_of_limitations:
                    if hist_begin <= terminal_date <= hist_horizon:
                        return True

        return False

    def _is_current(self, created_date, terminal_date, terminal_fate):
        if created_date <= self.time_horizon.as_ofF:
            if terminal_fate is None:
                return True
            else:
                if self.time_horizon.beginsF <= terminal_date < self.time_horizon.as_ofF:
                    if self.track_wins_for_hierarchy and terminal_fate == 'W':
                        return True
                    if self.track_losses_for_hierarchy and terminal_fate == 'L':
                        return True
        return False

    def is_active(self, record):
        return record.category == 'C'

    def is_history(self, record):
        return record.category == 'H'

    def is_closed(self, record):
        return record.category == 'I'

    def process_closure(self, record):
        return None

    def create_observations(self, record):
        return [record]

    def process_observation(self, active_record, observation):
        record = observation if observation else active_record
        for obs in record.observations:
            split_recs = self.get_splits(record, obs.amount)
            for split_rec in split_recs:
                splt_amt = split_rec.get("as_of_" + self.amt_fld, 0.0)
                if obs.category in ['HE', 'HN', 'HPN']:  # If is Historic Unborn -> Go into Hist Aggs
                    drilldown_vals = split_rec['path_tuple'] + tuple([obs.period_idx, obs.category == 'HE'])
                    for dim_flds_str, dim_vals_match_str, dim_idxs in self.debug_dims:
                        if obs.period_idx > 0:
                            # For debugging, only include records which count towards this forecast
                            ddv = '~'.join([str(drilldown_vals[i]) for i in dim_idxs])
                            if re.match(dim_vals_match_str, ddv):
                                self.debug_map["%s::%s" % (dim_flds_str, ddv)][record.ID].append(splt_amt)
                    self.hist_aggs[drilldown_vals].add('amount',
                                                       (record.created_date, splt_amt))
                    self.hist_aggs[drilldown_vals].add('count',
                                                       (record.created_date, 1))
                else:
                    drilldown_vals = split_rec['path_tuple']
                    self.actv_aggs[drilldown_vals].add('amount',
                                                       (self.th.as_ofF, splt_amt))
                    self.actv_aggs[drilldown_vals].add('count',
                                                       (self.th.as_ofF, 1))

    def get_total_results(self):
        new_results = {}
        existing_results = {}
        results = {}

        hist_aggs = self.hist_aggs
        actv_aggs = self.actv_aggs

        for agg in hist_aggs.values():
            agg.seal_for_query()

        # The index of the period field in the dim_vals.
        prd_idx = len(self.drilldown_flds)
        hist_df = pd.DataFrame([
            list(dim_vals) +
            [agg.get_as_of(fld, self.period_infos[int(dim_vals[prd_idx])][0])
             for fld in sorted(iter(agg.data.keys()))] +
            [agg.get_as_of(fld, self.period_infos[int(dim_vals[prd_idx])][1])
             for fld in sorted(iter(agg.data.keys()))]
            for dim_vals, agg in hist_aggs.items()
        ]
        )
        if hist_df.shape == (0, 0):
            logger.warning("WARNING: UnbornBaseModel did not find any historic deals... ")
            hist_df = pd.DataFrame([["."] * len(self.all_dim_flds) + [0, 0, 0, 0]])
        hist_df.columns = self.all_dim_flds + ('bop_amt', 'bop_cnt', 'amount', 'count')
        hist_df.sort_index(inplace=True)

        hist_existing_df = hist_df[hist_df['Existing'] == True]
        hist_df = hist_df[hist_df['Existing'] == False]

        hist_df = hist_df[hist_df['bop_amt'] > 0]

        for agg in actv_aggs.values():
            agg.seal_for_query()
        actv_df = pd.DataFrame([
            list(dim_vals) +
            [agg.get_as_of(k, self.th.as_ofF) for k in sorted(iter(agg.data.keys()))]
            for dim_vals, agg in actv_aggs.items()
        ],
        )
        if actv_df.shape == (0, 0):
            msg = "ERROR: UnbornBaseModel did not find any active deals... Something is not right!"
            logger.error(msg)
            if self.redist_scheme.get('name', 'none') != 'none':
                raise Exception(msg)
            actv_df = pd.DataFrame([["."] * len(self.all_dim_flds) + [0, 0, 0, 0]])
        actv_df.columns = self.drilldown_flds + ('amount', 'count')
        actv_df.sort_index(inplace=True)

        #we need to modify the distribution of the unborn if we're a future run
        if self.model_scheme['name'] in ['precomputed_mults', 'precomp_with_regional']:
            if self.produce_existing_results:
                ub_tot = self.runtime_params['scaled_ubf'] / (hist_df['amount'].sum() + hist_existing_df['amount'].sum())
                self.runtime_params['unborn_np'] = hist_df['amount'].sum() * ub_tot
                self.runtime_params['unborn_ep'] = hist_existing_df['amount'].sum() * ub_tot
            #otherwise just throw the full value in
            else:
                self.runtime_params['unborn_np'] = self.runtime_params['scaled_ubf']
                self.runtime_params['unborn_ep'] = 0

        hist_df = self.apply_model(hist_df, actv_df).reset_index()
        if self.produce_existing_results:
            hist_existing_df = self.apply_model(hist_existing_df, actv_df, newpipe_part=False).reset_index()

        bands_scheme = self.bands_scheme['name']

        def ssd(nparr):
            return mathUtils.ssd_fc(nparr, self.num_cycles)

        def calculate_result_for_drilldowns(res, prefix, df, dim_flds_key, dd_and_dim_flds):
            agg_df = df.groupby(list(dd_and_dim_flds)).agg({
                'amount': np.sum,
                'count': np.sum,
                'contrib_amt': np.sum,
                'bop_amt': np.sum,
                'bop_cnt': np.sum,
            }
            ).reset_index()
            if dim_flds_key == '.':
                tmp_df = pd.DataFrame()
                logger.info("Unborn Model Top Level Summary:\n%s", agg_df.to_string())
                tmp_df.loc['summary', 'fcst_mean'] = np.sum(agg_df['contrib_amt'])
                tmp_df.loc['summary', 'fcst_std'] = ssd(agg_df['amount'])
                agg_df = tmp_df
            else:
                agg_df = agg_df.groupby(list(drilldown_flds)).agg({
                    'contrib_amt':  np.sum,
                    'amount': ssd,
                }).rename(columns={'contrib_amt': 'fcst_mean', 'amount': 'fcst_std'})
            for grp_vals, grp_res in agg_df.iterrows():
                if self.viewgen_svc.hier_svc.cool_version:
                    dim_vals_key = '!' if dim_flds_key != '.' else dim_flds_key
                else:
                    dim_vals_key = '~'.join(str(x) for x in grp_vals) if isinstance(grp_vals, tuple) else str(grp_vals)
                fcst_mean = grp_res['fcst_mean']

                if bands_scheme == 'std_mults':
                    fcst_std = grp_res['fcst_std']
                    if math.isnan(fcst_std):
                        fcst_std = fcst_mean
                    lower_band = max(0, fcst_mean - self.bands_scheme['std_mults'][0] * fcst_std)
                    upper_band = fcst_mean + self.bands_scheme['std_mults'][1] * fcst_std
                elif bands_scheme == 'fixed_props':
                    lower_band = fcst_mean * self.bands_scheme['fixed_prop'][0]
                    upper_band = fcst_mean * self.bands_scheme['fixed_prop'][1]
                else:
                    raise Exception("bands_scheme '%s' not recognized" % (bands_scheme))
                if self.viewgen_svc.hier_svc.cool_version:
                    res_key = '.' if dim_flds_key == '.' else '!.' + grp_vals[-1]
                else:
                    res_key = '.' if dim_flds_key == '.' else (dim_flds_key + '.' + dim_vals_key)
                if res_key == '.' or not (lower_band == 0 and fcst_mean == 0 and upper_band == 0):
                    res[res_key] = {
                        prefix + 'down': lower_band,
                        prefix + 'value': fcst_mean,
                        prefix + 'up': upper_band,
                    }
            return

        logger.warning('DRILLS {}'.format(self.level_groupby_flds))
        for dim_flds_key, drilldown_flds in self.level_groupby_flds.items():
            calculate_result_for_drilldowns(new_results, '', hist_df, dim_flds_key,
                                            drilldown_flds + self.model_dim_flds)
            if self.produce_existing_results:
                calculate_result_for_drilldowns(existing_results, 'existing_', hist_existing_df,
                                                dim_flds_key, drilldown_flds + self.model_dim_flds)
        if self.produce_existing_results:
            results = merge_dict_of_dicts(new_results, existing_results)
        else:
            results = new_results

        if self.debug_dims:
            results['.']['DEBUG_INFO'] = {k: {kk: vv for kk, vv in v.items()}
                                          for k, v in self.debug_map.items()}
        return results

    def apply_model(self, hist_df, actv_df, newpipe_part=True):
        '''
        This method is expected to add a 'contrib_amt' field to buckets that
        it wants to have. The sum of contrib_amt should give the correct forecast
        without any further modifications. The 'amount' field will be used for
        calculating the bands. hist_df and actv_df are guaranteed to be passed in with
        reset index, and the hist_df returned is expected to have a rest index.

        we're throwing in unborn_part to log better the future runs
        '''
        hist_df = self.redistribute_amounts(hist_df, actv_df)
        model_scheme = self.model_scheme['name']
        if model_scheme == 'mean':
            rel_prds = list(range(self.num_cycles, 0, -1))
            prd_mults = pd.Series(np.ones(self.num_cycles), index=rel_prds)
        if model_scheme == 'cust_mults':
            prd_mults = sorted(self.model_scheme['cust_mults'].items(), reverse=True)
            prd_mults = pd.Series([x[1] for x in prd_mults],
                                  index=[abs(int(x[0])) for x in prd_mults])
        elif model_scheme == 'growth_extrap':
            growth_anchor_time = self.model_scheme['growth_anchor_time']
            if growth_anchor_time == 'bop':
                growth_amt_fld = 'bop_amt'
                time_idx = 0
            elif growth_anchor_time == 'as_of':
                growth_amt_fld = 'amount'
                time_idx = 1
            else:
                raise NotImplemented("growth_anchor_time '%s' not implemented" % growth_anchor_time)
            amt_by_prd = hist_df.groupby(self.model_dim_flds[0]).agg(
                {growth_amt_fld: np.sum}
            ).sort_index(ascending=False)
            growth_model = self.model_scheme['growth_model']
            if growth_model == 'geo_mean':
                from scipy.stats.mstats import gmean
                geo_mean = gmean((amt_by_prd[growth_amt_fld] / amt_by_prd[growth_amt_fld].shift(1)).dropna())
                if not np.isfinite(geo_mean):
                    logger.error("ERROR: Geometric mean is not finite, will default to 1.0")
                    geo_mean = 1.0
                rel_prds = list(range(self.num_cycles, 0, -1))
                prd_mults = pd.Series([np.power(geo_mean, i) for i in rel_prds],
                                      index=rel_prds)
            elif growth_model == 'linear':
                from sklearn.linear_model import LinearRegression
                model = LinearRegression()
                Xs = np.array([[self.period_infos[int(x)][time_idx]
                                for x in amt_by_prd.index.values]]).T
                Ys = amt_by_prd[growth_amt_fld]
                try:
                    model.fit(Xs, Ys)
                except:
                    logger.error("ERROR: Could not fit linear model. Possibly not enough data points." +
                                 "Defaulting to 0.0, 0.0.")
                    model.intercept_ = 0.0
                    model.coef_ = np.array([0.])
                logger.info("Unborn Model: Intercept=%s, Coeff=%s", model.intercept_, model.coef_)
                this_fcst = model.predict(self.period_infos[0][time_idx])[0]
                prd_mults = this_fcst / amt_by_prd[growth_amt_fld]
            else:
                raise NotImplemented("growth_model '%s' not implemented" % growth_model)
        elif model_scheme == 'seasonal_growth':
            prd_fld = self.model_dim_flds[0]
            offset = self.model_scheme.get('offset', 4)
            growth_amt_fld = 'bop_amt'
            amt_by_prd = hist_df.groupby(prd_fld).agg({growth_amt_fld: np.sum}).sort_index(ascending=False)

            from scipy.stats.mstats import gmean
            amt_ratio = amt_by_prd[growth_amt_fld] / amt_by_prd[growth_amt_fld].shift(offset)
            growth_rate = gmean(amt_ratio.dropna())
            on_season_tots = amt_by_prd[amt_by_prd.index.astype(int) % offset == 0][growth_amt_fld]
            tot_fcst = (on_season_tots * np.power(growth_rate, (on_season_tots.index.astype(int) / offset))).mean()
            #this might be bugged
            prd_mults = tot_fcst / amt_by_prd[growth_amt_fld]
        elif self.model_scheme['name'] == 'precomputed_mults':
            prd_fld = self.model_dim_flds[0]
            growth_amt_fld = 'amount'
            amt_by_prd = hist_df.groupby(prd_fld).agg({growth_amt_fld: np.sum}).sort_index(ascending=False)

            #not newpipe should be the unborn part of epf on future runs
            if newpipe_part:
                tot_fcst = self.runtime_params['unborn_np']
            else:
                tot_fcst = self.runtime_params['unborn_ep']
            logger.info('Unborn precomp - newpipe: {}, fcst: {}, total: {}'.format(newpipe_part, tot_fcst, self.runtime_params['scaled_ubf']))

            prd_mults = tot_fcst / amt_by_prd[growth_amt_fld]
            prd_mults[~np.isfinite(prd_mults)] = 1.0
        elif self.model_scheme['name'] == 'precomp_with_regional':
            region_fld = 'frozen_Level_0'
            prd_fld = self.model_dim_flds[0]
            grpby_flds = [region_fld, prd_fld]
            offset = self.model_scheme.get('offset', 4)

            if newpipe_part:
                tot_fcst = self.runtime_params['unborn_np']
                growth_amt_fld = 'bop_amt'
            else:
                tot_fcst = self.runtime_params['unborn_ep']
                growth_amt_fld = 'amount'

            logger.info('Unborn precomp - newpipe: {}, fcst: {}, total: {}'.format(newpipe_part, tot_fcst, self.runtime_params['scaled_ubf']))

            #get the amount won in each region by period
            base_df = hist_df.groupby(grpby_flds).agg({growth_amt_fld: np.sum}).sort_index(ascending=False)
            for_func = base_df.reset_index(level=region_fld)
            for_join = base_df[[growth_amt_fld]]

            #we should maybe rework this outside as staticmethod
            def ssnl_fcst(past_df, growth_amt_fld, offset):
                past_df = past_df.sort_index(ascending = False)
                amt_ratio = past_df[growth_amt_fld] / past_df[growth_amt_fld].shift(offset)
                from scipy.stats.mstats import gmean
                growth_rate = gmean(amt_ratio.dropna())
                on_season_tots = past_df[past_df.index.astype(int) % offset == 0][growth_amt_fld]
                tot_fcst = (on_season_tots * np.power(growth_rate, (on_season_tots.index.astype(int) / offset))).mean()
                return tot_fcst

            #do an individual seasonal model for each region
            reg_fcsts_unscaled = {}
            for reg in hist_df[region_fld].unique():
                reg_fcsts_unscaled[reg] = ssnl_fcst(for_func[for_func[region_fld] == reg], growth_amt_fld, offset)
            reg_fcsts_unscaled = pd.Series(reg_fcsts_unscaled)

            #rescale those so they equal the same total
            reg_fcsts = reg_fcsts_unscaled * tot_fcst / reg_fcsts_unscaled.sum()
            reg_fcsts = reg_fcsts.rename_axis(region_fld)
            reg_fcsts.name = 'reg_fcst'

            #get the right multiplier for each prd/reg
            mult_df = for_join.join(reg_fcsts).fillna(0)
            prd_mults = mult_df['reg_fcst'] / mult_df[growth_amt_fld]
            prd_mults.name = 'prd_mults'
            #because of different fill rules, were just gonna bail on this early
            hist_df = hist_df.join(prd_mults, on=grpby_flds)
            hist_df['prd_mults'].fillna(0, inplace = True)
            num_cycles = hist_df[hist_df[growth_amt_fld] > 0].groupby(region_fld)[prd_fld].nunique()
            num_cycles.name = 'num_cycles'
            hist_df = hist_df.join(num_cycles, on=[region_fld])
            hist_df['contrib_amt'] = hist_df['prd_mults'] * hist_df[growth_amt_fld] / hist_df['num_cycles']
            return hist_df

        prd_mults.fillna(1.0, inplace=True)
        logger.info("Unborn Model Period Multipliers:\n%s\n", prd_mults)
        hist_df['contrib_amt'] = hist_df['amount']
        for rel_prd, mult in prd_mults.items():
            hist_df.loc[hist_df['RelPrd'] == rel_prd, 'contrib_amt'] *= mult / self.num_cycles

        return hist_df

    @staticmethod
    def remove_forbidden_rows(df, forbidden_groups):
        """ Sample forbidden groups: ['frozen_Level_0~frozen_Level_1~::~EMEA~Benelux']"""
        for forbidden_group in forbidden_groups:
            flds, vals = tuple(x.split('~') for x in forbidden_group.split('~::~'))
            df = df[~reduce(lambda x, y: x & y,
                            [df.index.get_level_values(fld) == val for (fld, val) in zip(flds, vals)])]
        return df

    @cached_property
    def redist_hier(self):
        """ A list of strings that tells you which groupby fields to redistribute along. """
        redist_hier = self.redist_scheme.get('redist_hierarchy', [])
        if not redist_hier:
            redist_hier = []
        elif isinstance(redist_hier, basestring):
            redist_hier = redist_hier.split('~')
        else:
            if not isinstance(redist_hier, list):
                logger.error("ERROR: Unborn Model redist_hierarchy must be list of fields" +
                             "or tilde-separated string")
            for x in redist_hier:
                if len(x.split('~')) > 1:
                    logger.warning("WARNING: Unborn Model found tilde in field name for redist_hierarchy")
        redist_hier_available = [x for x in redist_hier if x in self.drilldown_flds]
        if len(redist_hier_available) != len(redist_hier):
            redist_hier = redist_hier_available
            logger.warning("WARNING: Unborn model will use %s for redistribution", redist_hier)
        return redist_hier

    def redistribute_amounts(self, hist_df, actv_df):
        '''
        redistribute the buckets as per the scheme configured.
        must return the redistributed hist_df with a reset index
        '''
        redist_scheme = self.redist_scheme['name']
        if redist_scheme == 'none' or not self.drilldown_flds:
            return hist_df.reset_index()

        if redist_scheme not in ['drop', 'proportional']:
            raise NotImplementedError("redist_scheme '%s' not implemented" % redist_scheme)

        hist_df = hist_df.set_index(list(self.drilldown_flds))
        actv_df = actv_df.set_index(list(self.drilldown_flds))

        if redist_scheme == 'drop':
            hist_df = hist_df[hist_df.index.isin(actv_df.index)]
            return hist_df.reset_index()

        #if the scheme is proportional, aka the one that has to do a bunch of work?
        prop_fld = self.redist_scheme.get('prop_fld', 'amount').lower()
        if prop_fld not in ['amount', 'count']:
            logger.error("Error: Unborn model prop field must be amount or count. Was %s instead.", prop_fld)
        redist_hier = self.redist_scheme.get('redist_hierarchy', [])
        if not redist_hier:
            redist_hier = []
        elif isinstance(redist_hier, basestring):
            redist_hier = redist_hier.split('~')
        else:
            if not isinstance(redist_hier, list):
                logger.error("ERROR: Unborn Model redist_hierarchy must be list of fields" +
                             "or tilde-separated string")
            for x in redist_hier:
                if len(x.split('~')) > 1:
                    logger.warning("WARNING: Unborn Model found tilde in field name for redist_hierarchy")
        redist_hier_available = [x for x in redist_hier if x in self.drilldown_flds]
        if len(redist_hier_available) != len(redist_hier):
            redist_hier = redist_hier_available
            logger.warning("WARNING: Unborn model will use %s for redistribution", redist_hier)
        if self.check_sanity:
            amt_by_prd_before = hist_df.groupby(list(self.model_dim_flds)).agg({'amount': np.sum}).sort_index()

        # No redist to is a list of nodes to which no forecast can
        # be redistributed, so we remove them from the actv_df.
        # Passing ['frozen_Level_0~frozen_Level_1~::~EMEA~Benelux']
        # means no redistributed forecast will go to Benelux.
        # TODO: Implement new version of this
        #no_redist_to = self.redist_scheme.get('no_redist_to', [])
        no_redist_to = []
        actv_df = UnbornBaseModel.remove_forbidden_rows(actv_df, no_redist_to)
        new_hist_df = UnbornBaseModel.remove_forbidden_rows(hist_df, no_redist_to)
        untouchable_df = hist_df[~hist_df.index.isin(new_hist_df.index)]
        hist_df = new_hist_df
        missing_df = hist_df[~hist_df.index.isin(actv_df.index)]
        ## TODO: ignore for now
        # no_redist_from = self.redist_scheme.get('no_redist_from', [])
        # missing_df = UnbornBaseModel.remove_forbidden_rows(missing_df, no_redist_from)

        missing_dd_flds = missing_df.index.names
        hist_df = hist_df[hist_df.index.isin(actv_df.index)]
        if self.redist_scheme.get('no_new_drilldowns', False):
            actv_df = actv_df[actv_df.index.isin(hist_df.index)]
        remerged = {}
        for missing_ddv, missing_vals in missing_df.iterrows():
            actv_subtot = 0
            actv_subdf = None
            missing_amt = missing_vals['amount']
            missing_cnt = missing_vals['count']
            missing_bop_amt = missing_vals['bop_amt']
            missing_model_dim_vals = tuple([self.model_dim_types[x](missing_vals[x])
                                            for x in self.model_dim_flds])
            if missing_amt > 0:
                for i in range(1, len(redist_hier) - 1):
                    found_ddk = tuple(redist_hier[:-i])
                    found_ddv = tuple([missing_ddv[missing_dd_flds.index(x)] for x in found_ddk])
                    try:
                        actv_subdf = actv_df.xs(found_ddv, level=found_ddk, drop_level=False)
                        actv_subtot = actv_subdf[prop_fld].sum()
                        if actv_subtot:
                            break
                    except:
                        pass
                else:
                    found_ddk = tuple()
                    found_ddv = tuple()
                if not found_ddk:
                    actv_subdf = actv_df
                    actv_subtot = actv_subdf[prop_fld].sum()
                found_ddk = found_ddk + self.model_dim_flds
                found_ddv = found_ddv + missing_model_dim_vals
                logger.info("will distribute %s opps worth $%s from bucket %s" +
                            " with model_dim_vals=%s to %s~%s (%s buckets)",
                            missing_cnt, missing_amt, missing_ddv,
                            missing_model_dim_vals, found_ddk, found_ddv, len(actv_subdf))
                if actv_subtot:
                    actv_subdf['Proportion'] = actv_subdf[prop_fld] / actv_subtot
                elif len(actv_subdf):
                    actv_subdf['Proportion'] = 1 / len(actv_subdf)
                else:
                    continue
                if self.check_sanity:
                    if abs(actv_subdf['Proportion'].sum() - 1) > 0.000000001:
                        logger.warning('WARNING: proportions adding up to %s', actv_subdf['Proportion'].sum())
                for actv_ddv, actv_vals in actv_subdf.iterrows():
                    prop = actv_vals['Proportion']
                    if isinstance(actv_ddv, basestring):
                        actv_ddv = (actv_ddv,)
                    actv_ddv = actv_ddv + missing_model_dim_vals
                    if actv_ddv not in remerged:
                        remerged[actv_ddv] = {
                            'amount': 0,
                            'count': 0,
                            'bop_amt': 0,
                            'bop_cnt': 0,
                        }
                    prop_amt = prop * missing_amt
                    if self.verbose_mode:
                        logger.info("\t%s += $%s", actv_ddv, prop_amt)
                    remerged[actv_ddv]['amount'] += prop_amt
                    remerged[actv_ddv]['count'] += 1
                    remerged[actv_ddv]['bop_amt'] += prop * missing_bop_amt
                    remerged[actv_ddv]['bop_cnt'] += 1
        # make sure model_dims are in the index for re-merging
        hist_df = hist_df.reset_index().set_index(list(self.all_dim_flds))
        untouchable_df = untouchable_df.reset_index().set_index(list(self.all_dim_flds))
        if remerged:
            if self.debug_dims:
                for ddv, rma in remerged.items():
                    for dim_flds_str, dim_vals_match_str, dim_idxs in self.debug_dims:
                        ddv = '~'.join([str(ddv[i]) for i in dim_idxs])
                        if re.match(dim_vals_match_str, ddv):
                            ddkv = "%s::%s" % (dim_flds_str, ddv)
                            recs = self.debug_map[ddkv]
                            if '__REDIST__' not in recs:
                                recs['__REDIST__'] = {'amount': 0, 'count': 0}
                            self.debug_map[ddkv]['__REDIST__']['amount'] += rma['amount']
                            self.debug_map[ddkv]['__REDIST__']['count'] += rma['count']
            remerged = pd.DataFrame(remerged).T
            remerged.index.names = list(self.all_dim_flds)
            hist_df = hist_df.add(remerged, fill_value=0)

        # concatenate
        hist_df = pd.concat([hist_df, untouchable_df])

        if self.check_sanity:
            amt_by_prd_after = hist_df.groupby(level=list(self.model_dim_flds)).agg(
                {'amount': np.sum}).sort_index()
            if abs(sum(amt_by_prd_before['amount'] - amt_by_prd_after['amount'])) > 0.001:
                logger.warning("WARNING: Unborn missing bucket distribution" +
                               " led to change in period totals:\n%s\n%s\n",
                               amt_by_prd_before, amt_by_prd_after)

        return hist_df.reset_index()

    # TODO: This needs to be fixed properly if we want to continue using this model
    def combine_results(self, results, combine_options):
        if len(results) > 1:
            raise Exception("ERROR: combine_results not implemented completely" +
                            ", you need to use overrides={'config':{'master_buffer_size':<some_big_number>}}")
        return results[0]

    def get_splits(self, record, amount):
        rec = {
            fld: record.get_dimension_valueF(fld, self.th, ubf_accounting=True)
            for fld in self.viewgen_svc.source_fields}
        rec["as_of_" + self.amt_fld] = amount
        return self.viewgen_svc.get_comps_with_paths(rec)


